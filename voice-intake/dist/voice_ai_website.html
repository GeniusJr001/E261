<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice AI Assistant</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: #0a0a0a;
            color: white;
            overflow: hidden;
        }

        /* Landing Page Styles */
        .landing-page {
            position: relative;
            width: 100vw;
            height: 100vh;
            overflow: hidden;
        }

        .video-container {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }

        .background-video {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }

        .video-overlay {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.3);
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            z-index: 10;
        }

        .video-controls {
            position: absolute;
            bottom: 30px;
            right: 30px;
            display: flex;
            gap: 15px;
            z-index: 20;
        }

        .control-btn {
            background: rgba(255, 255, 255, 0.2);
            border: 2px solid rgba(255, 255, 255, 0.3);
            color: white;
            padding: 12px 20px;
            border-radius: 25px;
            cursor: pointer;
            transition: all 0.3s ease;
            backdrop-filter: blur(10px);
            font-size: 14px;
            font-weight: 500;
        }

        .control-btn:hover {
            background: rgba(255, 255, 255, 0.3);
            border-color: rgba(255, 255, 255, 0.5);
            transform: translateY(-2px);
        }

        .start-btn {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            border: none;
            color: white;
            padding: 20px 40px;
            border-radius: 50px;
            cursor: pointer;
            font-size: 18px;
            font-weight: 600;
            transition: all 0.3s ease;
            box-shadow: 0 10px 30px rgba(102, 126, 234, 0.3);
        }

        .start-btn:hover {
            transform: translateY(-3px);
            box-shadow: 0 15px 40px rgba(102, 126, 234, 0.4);
        }

        /* Chat Page Styles */
        .chat-page {
            display: none;
            width: 100vw;
            height: 100vh;
            background: radial-gradient(ellipse at center, #1a1a2e 0%, #0a0a0a 100%);
            position: relative;
            overflow: hidden;
        }

        .chat-container {
            display: flex;
            flex-direction: column;
            height: 100vh;
            align-items: center;
            justify-content: center;
            padding: 20px;
        }

        .orb-container {
            position: relative;
            margin-bottom: 80px;
        }

        .voice-orb {
            width: 200px;
            height: 200px;
            border-radius: 50%;
            background: radial-gradient(circle at 30% 30%, #4facfe 0%, #00f2fe 100%);
            position: relative;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 0 50px rgba(79, 172, 254, 0.3);
            animation: pulse 2s infinite;
        }

        .voice-orb.listening {
            animation: listening 1.5s infinite;
            box-shadow: 0 0 80px rgba(79, 172, 254, 0.6);
        }

        .voice-orb.speaking {
            animation: speaking 0.8s infinite;
            box-shadow: 0 0 100px rgba(79, 172, 254, 0.8);
        }

        .voice-orb.processing {
            animation: processing 1s infinite;
        }

        .orb-inner {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            width: 140px;
            height: 140px;
            border-radius: 50%;
            background: rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(10px);
        }

        .orb-core {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            width: 80px;
            height: 80px;
            border-radius: 50%;
            background: rgba(255, 255, 255, 0.3);
        }

        @keyframes pulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.05); }
        }

        @keyframes listening {
            0%, 100% { transform: scale(1); box-shadow: 0 0 50px rgba(79, 172, 254, 0.3); }
            50% { transform: scale(1.1); box-shadow: 0 0 80px rgba(79, 172, 254, 0.6); }
        }

        @keyframes speaking {
            0%, 100% { transform: scale(1.05); }
            25% { transform: scale(1.15); }
            75% { transform: scale(0.95); }
        }

        @keyframes processing {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.1); }
        }

        .input-container {
            display: flex;
            align-items: center;
            gap: 15px;
            width: 100%;
            max-width: 600px;
            background: rgba(255, 255, 255, 0.1);
            border: 1px solid rgba(255, 255, 255, 0.2);
            border-radius: 25px;
            padding: 12px 20px;
            backdrop-filter: blur(10px);
        }

        .text-input {
            flex: 1;
            background: transparent;
            border: none;
            color: white;
            font-size: 16px;
            outline: none;
            padding: 8px 0;
        }

        .text-input::placeholder {
            color: rgba(255, 255, 255, 0.6);
        }

        .voice-btn {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            border: none;
            color: white;
            width: 40px;
            height: 40px;
            border-radius: 50%;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.3s ease;
        }

        .voice-btn:hover {
            transform: scale(1.1);
        }

        .voice-btn.recording {
            background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%);
            animation: recordPulse 1s infinite;
        }

        @keyframes recordPulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.1); }
        }

        .send-btn {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            border: none;
            color: white;
            width: 40px;
            height: 40px;
            border-radius: 50%;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.3s ease;
        }

        .send-btn:hover {
            transform: scale(1.1);
        }

        .status-text {
            /* Remove absolute positioning */
            /* position: absolute; */
            /* bottom: 120px; */
            /* Add relative positioning and margin for visibility after text bar */
            position: relative;
            margin-top: 20px;
            color: rgba(255, 255, 255, 0.7);
            font-size: 16px;
            text-align: center;
        }

        .particles {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            pointer-events: none;
            z-index: 1;
        }

        .particle {
            position: absolute;
            width: 2px;
            height: 2px;
            background: rgba(79, 172, 254, 0.3);
            border-radius: 50%;
            animation: float 10s infinite linear;
        }

        @keyframes float {
            0% {
                transform: translateY(100vh) rotate(0deg);
                opacity: 0;
            }
            10% {
                opacity: 1;
            }
            90% {
                opacity: 1;
            }
            100% {
                transform: translateY(-10vh) rotate(360deg);
                opacity: 0;
            }
        }

        /* Hidden class */
        .hidden {
            display: none !important;
        }

        /* Live Transcript Styles */
        .live-transcript {
            position: absolute;
            top: 20px;
            left: 50%;
            transform: translateX(-50%);
            background: rgba(255, 255, 255, 0.1);
            color: white;
            padding: 10px 20px;
            border-radius: 25px;
            backdrop-filter: blur(10px);
            font-size: 14px;
            font-weight: 500;
            z-index: 30;
        }
    </style>
</head>
<body>
    <!-- Landing Page -->
    <div id="landingPage" class="landing-page">
        <div class="video-container">
            <video id="backgroundVideo" class="background-video" autoplay muted>
                <!-- changed source to claim.mp4 (kept UI identical) -->
                <source src="./claim.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
        
        <div class="video-overlay">
            <button id="startBtn" class="start-btn">Start Voice Assistant</button>
        </div>
        
        <div class="video-controls">
            <button id="muteBtn" class="control-btn">üîä Unmute</button>
            <button id="skipBtn" class="control-btn">Skip ‚è≠</button>
        </div>
    </div>

    <!-- Chat Page -->
    <div id="chatPage" class="chat-page">
        <div class="particles" id="particles"></div>
        
        <div class="chat-container">
            <div class="orb-container">
                <div id="voiceOrb" class="voice-orb">
                    <div class="orb-inner">
                        <div class="orb-core"></div>
                    </div>
                </div>
            </div>
            
            <div id="statusText" class="status-text">Click the orb or press the microphone to start speaking</div>
            
            <!-- Add this to the chat page for live transcript -->
            <div id="liveTranscript" class="live-transcript">Live Transcript: ...</div>
            
            <div class="input-container">
                <input type="text" id="textInput" class="text-input" placeholder="Type your message or use voice...">
                <button id="voiceBtn" class="voice-btn">üé§</button>
                <button id="sendBtn" class="send-btn">‚û§</button>
            </div>
        </div>
    </div>

    <script>
        // Global variables
        let isRecording = false;
        let mediaRecorder;
        let audioChunks = [];
        let recognition;
        let isRecognizing = false;
        let accumulatedTranscript = '';  // New: accumulate transcripts across utterances
        let silenceTimeout;  // New: timeout for detecting end of speech
        let currentTimeout = 2500;  // Default silence timeout
        let audioContext = null;  // Global AudioContext for speaker access
        
        // DOM elements
        const landingPage = document.getElementById('landingPage');
        const chatPage = document.getElementById('chatPage');
        const backgroundVideo = document.getElementById('backgroundVideo');
        const muteBtn = document.getElementById('muteBtn');
        const skipBtn = document.getElementById('skipBtn');
        const startBtn = document.getElementById('startBtn');
        const voiceOrb = document.getElementById('voiceOrb');
        const voiceBtn = document.getElementById('voiceBtn');
        const sendBtn = document.getElementById('sendBtn');
        const textInput = document.getElementById('textInput');
        const statusText = document.getElementById('statusText');
        const liveTranscript = document.getElementById('liveTranscript');
        
        // backend base (dev -> localhost:8000, prod -> deployed API)
        const BACKEND = (location.hostname === 'localhost' || location.hostname === '127.0.0.1') ? 'http://127.0.0.1:8000' : 'https://e261-voice-backend.onrender.com';
         
         // Initialize particles
         function createParticles() {
             const particlesContainer = document.getElementById('particles');
             for (let i = 0; i < 50; i++) {
                 const particle = document.createElement('div');
                 particle.className = 'particle';
                 particle.style.left = Math.random() * 100 + '%';
                 particle.style.animationDelay = Math.random() * 10 + 's';
                 particle.style.animationDuration = (Math.random() * 10 + 10) + 's';
                 particlesContainer.appendChild(particle);
             }
         }
         
         // Video controls
         muteBtn.addEventListener('click', () => {
             if (backgroundVideo.muted) {
                 backgroundVideo.muted = false;
                 muteBtn.textContent = 'üîá Mute';
             } else {
                 backgroundVideo.muted = true;
                 muteBtn.textContent = 'üîä Unmute';
             }
         });
         
         skipBtn.addEventListener('click', () => {
             // Immediately stop the intro video and any background sounds, then go to chat
            try {
                // pause and jump to end to stop playback
                backgroundVideo.pause();
                backgroundVideo.currentTime = backgroundVideo.duration || 0;
                // mute and zero volume as extra guarantee
                backgroundVideo.muted = true;
                backgroundVideo.volume = 0;
            } catch (e) {
                console.warn('Failed to stop background video', e);
            }

            // Suspend any AudioContext created on the page (intro_skip.js may have created one)
            try {
                if (window.__introAudioCtx && typeof window.__introAudioCtx.suspend === 'function') {
                    window.__introAudioCtx.suspend().catch(()=>{});
                }
            } catch (e) { /* ignore */ }

            // Stop any other <audio> elements that might be playing (TTS previews, etc.)
            try {
                document.querySelectorAll('audio').forEach(a => {
                    try { a.pause(); a.currentTime = 0; a.muted = true; } catch(_) {}
                });
            } catch (e) {}

            // Finally transition to chat
            showChatPage();
         });
         
        // startBtn visibility is controlled so it only appears after the video ends
        startBtn.addEventListener('click', () => {
            showChatPage();
        });
         
         // Page transition
         function showChatPage() {
             landingPage.classList.add('hidden');
             chatPage.style.display = 'block';
             createParticles();
             initializeVoiceRecognition();
            // Remove automatic start here; will start after initial TTS
            // Hide the voice button since recognition is continuous
            voiceBtn.style.display = 'none';
            // optionally notify backend that session started
            startConversation().catch(()=>{});
         }
         
        // Initialize audio context and request speaker access
        async function initializeAudioAccess() {
            try {
                // Create AudioContext to ensure audio playback capability
                const AudioCtx = window.AudioContext || window.webkitAudioContext;
                if (AudioCtx && !audioContext) {
                    audioContext = new AudioCtx();
                    // Resume context if suspended (required by some browsers)
                    if (audioContext.state === 'suspended') {
                        await audioContext.resume();
                    }
                    console.log('[AUDIO] AudioContext initialized, state:', audioContext.state);
                }
                
                // Test audio playback capability with a silent buffer
                if (audioContext) {
                    const buffer = audioContext.createBuffer(1, 1, 22050);
                    const source = audioContext.createBufferSource();
                    source.buffer = buffer;
                    source.connect(audioContext.destination);
                    source.start();
                    console.log('[AUDIO] Test audio played successfully');
                }
                
                return true;
            } catch (error) {
                console.warn('[AUDIO] Failed to initialize audio access:', error);
                return false;
            }
        }
         
        async function startConversation() {
            try {
                console.log('[START] Requesting mic access');
                // Request mic access immediately after user gesture (before TTS)
                await navigator.mediaDevices.getUserMedia({ audio: true });
                console.log('[START] Mic access granted');
                
                // Initialize audio context for speaker access
                await initializeAudioAccess();
                
                const res = await fetch(`${BACKEND}/conversation/start`, { method: 'POST' });
                if (!res.ok) return;
                const data = await res.json();
                sessionId = data.session_id || data.sessionId || null;
                if (data.prompt) {
                    // Hide the TTS text: don't set statusText to data.prompt
                    // statusText.textContent = data.prompt;
                    statusText.textContent = 'Starting conversation...';  // Optional: Set a generic message
                    // Play TTS and wait for it to fully end (playTTS handles resuming recognition)
                    await playTTS(data.prompt);
                    console.log('[START] TTS fully ended');
                    // Remove redundant recognition.start() here, as playTTS handles it
                }
                // Set initial timeout from backend
                currentTimeout = data.silence_timeout || 2500;
            } catch (e) {
                console.warn('Mic access denied or startConversation failed', e);
                statusText.textContent = 'Microphone access required. Please allow and refresh.';
            }
        }
         // Ensure globals used elsewhere exist
  window.recognition = window.recognition || null;
  window.isRecognizing = window.isRecognizing || false;

  // Minimal voice recognition initializer (safe no-op if API missing)
  function initializeVoiceRecognition() {
    if (window.recognition) return; // already initialized
    const SpeechRec = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRec) {
      console.warn('SpeechRecognition not available in this browser');
      return;
    }

    try {
      window.recognition = new SpeechRec();
      window.recognition.continuous = true;  // Changed: Enable continuous recognition
      window.recognition.interimResults = true;
      window.recognition.lang = 'en-US';

      window.recognition.onstart = () => {
        console.log('[REC] Recognition started');
        window.isRecognizing = true;
        const orb = document.getElementById('voiceOrb');
        if (orb) orb.classList.add('listening');
        try { document.getElementById('statusText').textContent = 'Listening continuously...'; } catch(e){}
      };

      window.recognition.onresult = (event) => {
        console.log('[REC] Result received');
        let interimTranscript = '';
        let finalTranscript = '';
        for (let i = event.resultIndex; i < event.results.length; i++) {
            const transcript = event.results[i][0].transcript;
            if (event.results[i].isFinal) {
                finalTranscript += transcript;
            } else {
                interimTranscript += transcript;
            }
        }
        if (finalTranscript) {
            accumulatedTranscript += finalTranscript;  // Accumulate final transcripts
            // Clear any existing silence timeout
            if (silenceTimeout) clearTimeout(silenceTimeout);
            // Set a new timeout to detect end of speech (use currentTimeout)
            silenceTimeout = setTimeout(() => {
                if (accumulatedTranscript.trim()) {
                    sendTextAsUser(accumulatedTranscript.trim());
                    accumulatedTranscript = '';
                    voiceOrb.classList.add('processing');  // New processing state
                }
            }, currentTimeout);
        }
        // Remove update to text input; only show in live transcript above orb
        // const input = document.getElementById('textInput');
        // if (input) input.value = accumulatedTranscript + interimTranscript;

        // Update live transcript display
        const liveTranscript = document.getElementById('liveTranscript');
        if (liveTranscript) {
            liveTranscript.textContent = 'Live Transcript: ' + accumulatedTranscript + interimTranscript;
        }

        // Update status with current timeout
        statusText.textContent = 'Listening... (Sending in ' + (currentTimeout / 1000) + 's)';
      };

      window.recognition.onerror = (ev) => {
        console.warn('[REC] Error:', ev.error);
        const orb = document.getElementById('voiceOrb');
        if (orb) orb.classList.remove('listening');
        try { document.getElementById('statusText').textContent = 'Click the orb to start listening'; } catch(e){}
        // Reset accumulation on error
        accumulatedTranscript = '';
        if (silenceTimeout) clearTimeout(silenceTimeout);
      };

      window.recognition.onend = () => {
        console.log('[REC] Recognition ended');
        window.isRecognizing = false;
        const orb = document.getElementById('voiceOrb');
        if (orb) orb.classList.remove('listening');
        try { document.getElementById('statusText').textContent = 'Click the orb to start listening'; } catch(e){}
        // Reset accumulation
        accumulatedTranscript = '';
        if (silenceTimeout) clearTimeout(silenceTimeout);
      };
    } catch (e) {
      console.error('initializeVoiceRecognition failed', e);
    }
  }
         
         // Audio recording setup
         async function startRecording() {
             try {
                 const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                 mediaRecorder = new MediaRecorder(stream);
                 audioChunks = [];
                 
                 mediaRecorder.ondataavailable = (event) => {
                     audioChunks.push(event.data);
                 };
                 
                 mediaRecorder.onstop = () => {
                    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                    // ensure we have a session id; startConversation will set sessionId if needed
                    (async () => {
                      if (!sessionId) {
                        await startConversation();
                      }
                      sendAudioToBackend(audioBlob);
                    })();
                     stream.getTracks().forEach(track => track.stop());
                 };
                 
                 mediaRecorder.start();
                 isRecording = true;
                 voiceBtn.classList.add('recording');
                 voiceOrb.classList.add('listening');
                 statusText.textContent = 'Recording...';
                 
             } catch (error) {
                 console.error('Error accessing microphone:', error);
                 statusText.textContent = 'Microphone access denied';
             }
         }
         
         function stopRecording() {
             if (mediaRecorder && isRecording) {
                 mediaRecorder.stop();
                 isRecording = false;
                 voiceBtn.classList.remove('recording');
                 voiceOrb.classList.remove('listening');
                 statusText.textContent = 'Processing...';
             }
         }
         
         // Send audio to backend
         async function sendAudioToBackend(audioBlob) {
        const formData = new FormData();
        // server expects field named "file"
        formData.append('file', audioBlob, 'recording.webm');
        try {
            if (!sessionId) {
                // create a session if missing
                await startConversation();
                if (!sessionId) throw new Error('no sessionId available');
            }
            const url = `${BACKEND}/conversation/respond?session_id=${encodeURIComponent(sessionId)}`;
            console.log('[PARENT] posting audio to', url, 'form keys=', [...formData.keys()]);
            const response = await fetch(url, { method: 'POST', body: formData });

            // ensure we always clear recording/listening UI when we get a response (or error)
            const clearListening = () => { try { voiceOrb.classList.remove('listening'); voiceOrb.classList.remove('speaking'); } catch(e){} };

            if (!response.ok) {
                // try to show backend message if available
                let bodyText = await response.text().catch(()=>response.statusText);
                console.error('Backend error:', response.status, bodyText);
                statusText.textContent = 'Error processing audio';
                clearListening();
                return;
            }

            const contentType = response.headers.get('content-type') || '';

            if (contentType.includes('application/json')) {
                // backend returned JSON: { next_prompt, collected, done, session_id }
                const data = await response.json();
                console.log('[PARENT] conversation/respond JSON', data);

                // Update sessionId if backend returned one
                if (data.session_id) sessionId = data.session_id;

                const nextPrompt = data.next_prompt || data.prompt || data.response || '';
                const done = !!data.done;

                if (nextPrompt) {
                    // update UI and ask TTS to speak the prompt
                    statusText.textContent = nextPrompt;
                    clearListening();
                    // play TTS from backend (uses /tts endpoint)
                    try {
                        await playTTS(nextPrompt);
                    } catch (e) {
                        console.warn('playTTS failed', e);
                    }
                } else {
                    // no prompt returned: clear UI state
                    statusText.textContent = done ? 'Done.' : 'Response received';
                    clearListening();
                }

            } else {
                // backend returned binary audio (blob) ‚Äî play it
                const blob = await response.blob();
                console.log('[PARENT] conversation/respond returned audio blob size=', blob.size);
                playAudioResponse(blob);
            }
        } catch (error) {
            console.error('Network error:', error);
            statusText.textContent = 'Network error';
            try { voiceOrb.classList.remove('listening'); voiceOrb.classList.remove('speaking'); } catch(e){}
        }
    }
         
         // Send text message -> POST to TTS endpoint (http://127.0.0.1:8000/tts)
         async function sendMessage(message) {
             if (!message.trim()) return;

             voiceOrb.classList.add('speaking');
             statusText.textContent = 'Processing...';

             try {
                 const res = await fetch(`${BACKEND}/tts`, {
                     method: 'POST',
                     headers: { 'Content-Type': 'application/json' },
                     body: JSON.stringify({ sessionId, text: message })
                 });
                 if (!res.ok) {
                     console.error('Backend error:', res.statusText);
                     statusText.textContent = 'Error processing message';
                     voiceOrb.classList.remove('speaking');
                     return;
                 }

                 // backend expected to return audio blob (or JSON). handle both.
                 const contentType = res.headers.get('content-type') || '';
                 if (contentType.includes('application/json')) {
                     const data = await res.json();
                     // handle JSON response (e.g. { url: "...", base64: "..." })
                     console.log('TTS returned JSON:', data);
                 } else {
                     const blob = await res.blob();
                     // play received audio blob
                     const audioUrl = URL.createObjectURL(blob);
                     const audio = new Audio(audioUrl);
                     audio.onended = () => {
                         URL.revokeObjectURL(audioUrl);
                         voiceOrb.classList.remove('speaking');
                         statusText.textContent = '';
                     };
                     audio.onerror = (e) => {
                         console.error('Audio playback error', e);
                         voiceOrb.classList.remove('speaking');
                         statusText.textContent = 'Playback error';
                     };
                     audio.play().catch(err => {
                         console.error('Play() failed:', err);
                         voiceOrb.classList.remove('speaking');
                         statusText.textContent = 'Playback failed';
                     });
                 }
             } catch (err) {
                 console.error('Network error sending TTS:', err);
                 statusText.textContent = 'Network error';
                 voiceOrb.classList.remove('speaking');
             }
         }
         
         // Handle backend response
         function handleBackendResponse(response) {
             voiceOrb.classList.remove('speaking', 'listening');
             
             // If backend returns audio response
             if (response.audioUrl) {
                 playAudioResponse(response.audioUrl);
             }
             
             // Update status
             if (response.status) {
                 statusText.textContent = response.status;
             } else if (response.response) {
                 statusText.textContent = response.response;
             } else {
                 statusText.textContent = 'Click the orb or press the microphone to start speaking';
             }
         }
         
         // Play audio response from backend
        async function playTTS(text) {
            if (!text) return;
            console.log('[TTS] Starting for text:', text);
            try {
                // ensure/resume AudioContext
                if (!audioContext) {
                    const AudioCtx = window.AudioContext || window.webkitAudioContext;
                    audioContext = AudioCtx ? new AudioCtx() : null;
                }
                if (audioContext && audioContext.state === 'suspended') {
                    await audioContext.resume();
                    console.log('[TTS] AudioContext resumed');
                }

                // pause recognition while speaking
                if (window.recognition && window.isRecognizing) {
                    try { window.recognition.stop(); } catch(_) {}
                }

                const resp = await fetch(`${BACKEND}/tts`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ text })
                });
                if (!resp.ok) throw new Error('TTS fetch failed: ' + resp.status);

                const arrayBuffer = await resp.arrayBuffer();
                console.log('[TTS] arrayBuffer byteLength=', arrayBuffer.byteLength);

                // debug: log first 64 bytes as hex
                try {
                    const view = new Uint8Array(arrayBuffer.slice(0, 64));
                    console.log('[TTS] first64hex=', Array.from(view).map(b => b.toString(16).padStart(2,'0')).join(''));
                } catch (e) { /* ignore */ }

                if (!audioContext) {
                    // fallback to audio element if AudioContext not available
                    const blob = new Blob([arrayBuffer], { type: resp.headers.get('content-type') || 'audio/wav' });
                    const url = URL.createObjectURL(blob);
                    const a = new Audio(url);
                    a.volume = 1.0; a.muted = false;
                    await a.play().catch(e => console.warn('[TTS] fallback play failed', e));
                    return;
                }

                // decode & play via AudioContext (more reliable)
                const decoded = await audioContext.decodeAudioData(arrayBuffer.slice(0));
                const src = audioContext.createBufferSource();
                src.buffer = decoded;
                src.connect(audioContext.destination);

                voiceOrb.classList.add('speaking');
                statusText.textContent = '';

                await new Promise((resolve) => {
                    src.onended = () => {
                        voiceOrb.classList.remove('speaking');
                        if (window.recognition && !window.isRecognizing) {
                            try { window.recognition.start(); } catch(_) {}
                        }
                        resolve();
                    };
                    try { src.start(0); } catch (e) { console.warn('[TTS] src.start failed', e); resolve(); }
                });

            } catch (err) {
                console.error('[TTS] Error:', err);
                voiceOrb.classList.remove('speaking');
                if (window.recognition && !window.isRecognizing) {
                    try { window.recognition.start(); } catch(_) {}
                }
            }
        }

        function playAudioResponse(src) {
            let audioEl;
            try {
                if (!src) return;
                if (src instanceof Blob) {
                    const url = URL.createObjectURL(src);
                    audioEl = new Audio(url);
                    audioEl.onended = () => { URL.revokeObjectURL(url); voiceOrb.classList.remove('speaking'); };
                } else if (src instanceof ArrayBuffer) {
                    const blob = new Blob([src], { type: 'audio/webm' });
                    const url = URL.createObjectURL(blob);
                    audioEl = new Audio(url);
                    audioEl.onended = () => { URL.revokeObjectURL(url); voiceOrb.classList.remove('speaking'); };
                } else if (typeof src === 'string') {
                    audioEl = new Audio(src);
                    audioEl.onended = () => voiceOrb.classList.remove('speaking');
                } else {
                    console.warn('Unsupported audio src', src);
                    return;
                }
                audioEl.onplay = () => voiceOrb.classList.add('speaking');
                audioEl.onerror = (e) => { console.error('Error playing audio:', e); voiceOrb.classList.remove('speaking'); };
                audioEl.play().catch(err => {
                    console.warn('Audio play rejected:', err);
                    try { const ac = new (window.AudioContext || window.webkitAudioContext)(); ac.resume().then(()=>audioEl.play().catch(()=>{})); } catch(e){}
                });
            } catch (e) {
                console.error('playAudioResponse error', e);
            }
        }
         
         // Event listeners
         voiceOrb.addEventListener('click', () => {
             // Start continuous recognition if not already running
             if (window.recognition && !window.isRecognizing) {
                 try {
                     window.recognition.start();
                 } catch (e) {
                     console.warn('recognition.start failed', e);
                 }
             }
         });
         
         // Send text as a user response to the conversation endpoint (same flow as recorded audio)
         async function sendTextAsUser(message) {
             if (!message || !message.trim()) return;
             if (!sessionId) {
                 await startConversation();
                 if (!sessionId) return;
             }
             voiceOrb.classList.add('speaking');
             statusText.textContent = 'Processing...';
             try {
                 const url = `${BACKEND}/conversation/respond?session_id=${encodeURIComponent(sessionId)}`;
                 console.log('[PARENT] sendTextAsUser ->', url, 'message=', message);
                 const res = await fetch(url, {
                     method: 'POST',
                     headers: { 'Content-Type': 'application/json' },
                     body: JSON.stringify({ text: message })
                 });
                 console.log('[PARENT] response status', res.status);

                 if (!res.ok) {
                     const body = await res.text().catch(()=>res.statusText);
                     console.error('Backend error sending text:', res.status, body);
                     statusText.textContent = 'Error processing message';
                     voiceOrb.classList.remove('speaking');
                     return;
                 }

                 const data = await res.json();
                 console.log('[PARENT] conversation/respond JSON', data);
                 
                 // Check if data is null to prevent TypeError
                 if (!data) {
                     console.error('Backend returned null');
                     statusText.textContent = 'Error: Backend returned null';
                     voiceOrb.classList.remove('speaking');
                     return;
                 }
                 
                 // Set timeout from backend
                 currentTimeout = data.silence_timeout || 2500;
                 // prefer next_prompt, fallback to prompt/response
                 const nextPrompt = data.next_prompt || data.prompt || data.response || '';
                 // update UI and request TTS for the prompt if present
                 if (nextPrompt) {
                     statusText.textContent = nextPrompt;
                     // play TTS from backend (uses /tts endpoint)
                     try {
                         await playTTS(nextPrompt);
                     } catch (e) {
                         console.warn('playTTS failed', e);
                     }
                 } else {
                     statusText.textContent = 'Response received';
                     voiceOrb.classList.remove('speaking');
                 }

             } catch (err) {
                 console.error('Network error sending conversation text:', err);
                 statusText.textContent = 'Network error';
                 voiceOrb.classList.remove('speaking');
             } finally {
                 // clear input
                 textInput.value = '';
             }
         }

         // wire send button and Enter key to sendTextAsUser (same effect as recording)
         sendBtn.addEventListener('click', () => {
             sendTextAsUser(textInput.value);
         });
         
         textInput.addEventListener('keypress', (e) => {
             if (e.key === 'Enter') {
                 e.preventDefault();
                 sendTextAsUser(textInput.value);
             }
         });
         
         // Initialize
         document.addEventListener('DOMContentLoaded', () => {
            // keep UI unchanged visually: hide start button until the video has ended
            startBtn.style.display = 'none';
            // when the video ends, reveal the start button (ensures video plays/ends first)
            backgroundVideo.addEventListener('ended', () => {
                startBtn.style.display = '';
            });
            // also try to autoplay but don't show start before end
            backgroundVideo.play().catch(() => {});
         });
     </script>
</body>
</html>
